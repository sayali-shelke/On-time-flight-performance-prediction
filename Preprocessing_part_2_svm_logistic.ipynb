{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = pd.read_csv('dfw.csv')\n",
    "atl = pd.read_csv('atl.csv')\n",
    "ewr = pd.read_csv('ewr.csv')\n",
    "clt = pd.read_csv('clt.csv')\n",
    "den = pd.read_csv('den.csv')\n",
    "weather = pd.concat([dfw,atl,ewr,clt,den])\n",
    "flight = pd.read_csv(r'df14final.csv')\n",
    "#weather = pd.read_csv(r'2014_df.csv')#merging all the weather data by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight['DEP_DateTime'] = pd.to_datetime(flight['DEP_DateTime'], errors='coerce')\n",
    "flight['DEP_DateTime'] = flight['DEP_DateTime'].dt.round('H').dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight['DEP_DateTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight['DEP_DateTime'] = pd.to_datetime(flight['DEP_DateTime'], format='%H').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight['DEP_DateTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmerge14['ARR_DateTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['end_Time'] = weather['end_Time'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight['DEP_datetime'] = pd.to_datetime(flight['FL_DATE'].apply(str)+' '+flight['DEP_DateTime'].apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['end_Time'] = pd.to_datetime(weather['end_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight['DEP_datetime'] = flight['DEP_datetime'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmerge14=pd.merge(final_weather, flight, left_on=['place','end_Time'],right_on=['ORIGIN','DEP_datetime'],how='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmerge14['ARR_DateTime'] = pd.to_datetime(finalmerge14['ARR_DateTime'], errors='coerce')\n",
    "finalmerge14['ARR_DateTime'] = finalmerge14['ARR_DateTime'].dt.round('H').dt.hour\n",
    "finalmerge14['ARR_DateTime'] = pd.to_datetime(finalmerge14['ARR_DateTime'], format='%H').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmerge14['ARR_datetime'] = pd.to_datetime(finalmerge14['FL_DATE'].apply(str)+' '+finalmerge14['ARR_DateTime'].apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmerge14['ARR_datetime'] = finalmerge14['ARR_datetime'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmerge14_2=pd.merge(final_weather, finalmerge14, left_on=['place','end_Time'],right_on=['DEST','ARR_datetime'],how='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.rename({'DEP_datetime3':'end_Time'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#finalmerge14['ARR_TIME_y'] = finalmerge14['ARR_TIME_y'].map(lambda x: x.lstrip('0 days').rstrip('aAbBcC'))\n",
    "#finalmerge14['CRS_ARR_TIME_y'] = finalmerge14['CRS_ARR_TIME_y'].map(lambda x: x.lstrip('0 days').rstrip('aAbBcC'))\n",
    "#finalmerge14['CRS_ARR_TIME_y'] = datetime.datetime.strptime(finalmerge14['ARR_TIME_y'], '%H:%M:%S.%f')\n",
    "#finalmerge14['ARR_TIME_y'] = datetime.datetime.strptime(finalmerge14['CRS_ARR_TIME_y'], '%H:%M:%S.%f')\n",
    "#finalmerge14['ARR_TIME_y'] = finalmerge14['CRS_ARR_TIME_y'].map(lambda x: x.lstrip('00000').rstrip('aAbBcC'))\n",
    "finalmerge14['ARR_TIME_y'] = finalmerge14['ARR_TIME_z']\n",
    "finalmerge14['CRS_ARR_TIME_y'] = finalmerge14['CRS_ARR_TIME_z']\n",
    "finalmerge14['ARR_TIME_y']= pd.to_datetime(finalmerge14['ARR_TIME_y'],errors= 'coerce')\n",
    "finalmerge14['CRS_ARR_TIME_y']= pd.to_datetime(finalmerge14['CRS_ARR_TIME_y'],errors= 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmerge14.loc[finalmerge14['ARR_TIME_y'] > finalmerge14['CRS_ARR_TIME_y'], 'delay'] = finalmerge14['ARR_TIME_y']-finalmerge14['CRS_ARR_TIME_y']\n",
    "finalmerge14.loc[finalmerge14['ARR_TIME_y'] <= finalmerge14['CRS_ARR_TIME_y'], 'delay'] = 0\n",
    "finalmerge14['delay']\n",
    "\n",
    "#finalmerge14['delay']=finalmerge14['delay'].astype('timedelta64[m]')\n",
    "l=[]\n",
    "for minutes in finalmerge14['delay']:\n",
    "    minutes= pd.to_timedelta(minutes)\n",
    "    minutes = minutes.total_seconds() / 60   \n",
    "    l.append(minutes) \n",
    "finalmerge14.drop(labels=['delay'],inplace=True,axis=1)\n",
    "finalmerge14=finalmerge14.assign(delay=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(finalmerge14[finalmerge14['ARR_TIME_y'].isnull()])\n",
    "finalmerge14['ARR_TIME_y'] = pd.to_numeric(finalmerge14['ARR_TIME_y'], errors='coerce')\n",
    "finalmerge14['CRS_ARR_TIME_y'] = pd.to_numeric(finalmerge14['CRS_ARR_TIME_y'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "s = pd.read_csv('new_final.csv')#merged excel file. \n",
    "#In the excel file, remove the '0 days' and '1 days' and the nano second and perfrom the operations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "s['ARR_TIME_y'] = s['ARR_TIME_y'].map(lambda x: x.lstrip(' ').rstrip('aAbBcC'))\n",
    "s['CRS_ARR_TIME_y'] = s['CRS_ARR_TIME_y'].map(lambda x: x.lstrip(' ').rstrip('aAbBcC'))\n",
    "#finalmerge14['CRS_ARR_TIME_y'] = datetime.datetime.strptime(finalmerge14['ARR_TIME_y'], '%H:%M:%S.%f')\n",
    "#finalmerge14['ARR_TIME_y'] = datetime.datetime.strptime(finalmerge14['CRS_ARR_TIME_y'], '%H:%M:%S.%f')\n",
    "#finalmerge14['ARR_TIME_y'] = finalmerge14['CRS_ARR_TIME_y'].map(lambda x: x.lstrip('00000').rstrip('aAbBcC'))\n",
    "\n",
    "s['ARR_TIME_y']= pd.to_datetime(s['ARR_TIME_y'],errors= 'coerce')\n",
    "s['CRS_ARR_TIME_y']= pd.to_datetime(s['CRS_ARR_TIME_y'],errors= 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import timedelta  \n",
    "\n",
    "q = np.where(s['CRS_ARR_TIME_y'].dt.hour - s['ARR_TIME_y'].dt.hour>15)\n",
    "\n",
    "j=-1\n",
    "for i in q[0]:\n",
    "    j+=1\n",
    "    print(i)\n",
    "    s['ARR_TIME_y'][i]= s['ARR_TIME_y'][i] + timedelta(days=1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.drop(['target'],axis=1)\n",
    "s.loc[s['ARR_TIME_y'] > s['CRS_ARR_TIME_y'], 'target'] = 1\n",
    "s.loc[s['ARR_TIME_y'] <= s['CRS_ARR_TIME_y'], 'target'] = 0\n",
    "s['target'] = s['target'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[s['ARR_TIME_y'] > s['CRS_ARR_TIME_y'], 'delay'] = s['ARR_TIME_y']-s['CRS_ARR_TIME_y']\n",
    "s.loc[s['ARR_TIME_y'] <= s['CRS_ARR_TIME_y'], 'delay'] = 0\n",
    "\n",
    "#finalmerge14['delay']=finalmerge14['delay'].astype('timedelta64[m]')\n",
    "l=[]\n",
    "for minutes in s['delay']:\n",
    "    minutes= pd.to_timedelta(minutes)\n",
    "    minutes = minutes.total_seconds() / 60   \n",
    "    l.append(minutes) \n",
    "s.drop(labels=['delay'],inplace=True,axis=1)\n",
    "s=s.assign(delay=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[s['ARR_TIME_y'] > s['CRS_ARR_TIME_y'], 'target'] = 1\n",
    "s.loc[s['ARR_TIME_y'] <= s['CRS_ARR_TIME_y'], 'target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv('new_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ss.drop(['Unnamed: 0'],axis=1)\n",
    "s3 = ss\n",
    "s3 = s3.loc[:,s3.columns!='target']\n",
    "s3 = s3.loc[:,s3.columns!='delay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_mask = s3.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = s3.columns[categorical_feature_mask].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# instantiate labelencoder object\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3[categorical_cols] = s3[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "s3[categorical_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ss['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split the data and avoid snooping\n",
    "X = s3\n",
    "X =X.fillna(0)\n",
    "X_test = X.tail(15000) # comment for cross validation\n",
    "X = X.head(82430)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "y_test = y.tail(15000) # Comment for cross validation\n",
    "y=y.head(82430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Random data testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorr = []\n",
    "b = int(X.shape[0]/5)\n",
    "# To perform K-fold, uncomment the X_test and y_test \n",
    "#To perform on a model, just replace the model definition and do a .fit in the loop\n",
    "#perform pca transform only if computation is poor\n",
    "f1=[]\n",
    "for i in range(4):\n",
    "    d = b*i\n",
    "    print(d)\n",
    "    #X_test = X.iloc[0+d:int(d+y.shape[0]/5)]\n",
    "    X_train = X.drop(X.index[0+d:int(d+X.shape[0]/5)])\n",
    "    #y_test = y.iloc[0+d:int(d+y.shape[0]/5)]\n",
    "    y_train = y.drop(y.index[0+d:int(d+X.shape[0]/5)])\n",
    "    #scaler = StandardScaler()\n",
    "\n",
    "    #scaler.fit(X_train)\n",
    "    #train_img = scaler.transform(X_train)\n",
    "    #test_img = scaler.transform(X_test)\n",
    "    #pca = PCA(0.80)\n",
    "    #pca.fit(train_img)\n",
    "    #train_img = pca.transform(train_img)\n",
    "    #test_img = pca.transform(test_img)\n",
    "    #svclassifier4 = SVC(C=200,kernel='rbf',gamma='scale')\n",
    "    #svclassifier4.fit(train_img, y_train)\n",
    "    logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "    scorr.append(logisticRegr.score(X_test, y_test))\n",
    "    y_pred = logisticRegr.predict(X_test)\n",
    "    f1.append(f1_score(y_test, y_pred))  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalising the data \n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "train_img = scaler.transform(X_train)\n",
    "test_img = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=12)\n",
    "pca.fit(train_img)\n",
    "pca.n_components_\n",
    "#Displaying the linear combination data\n",
    "from sklearn.decomposition import PCA\n",
    "principalComponents = pca.fit_transform(train_img)\n",
    "#principalDf = pd.DataFrame(data = principalComponents\n",
    "#            , columns = ['principal component 1', 'principal component 2','principal component 3','principal component 4', 'principal component 5','principal component 6','principal component 7', 'principal component 8','principal component 9','principal component 10', 'principal component 11','principal component 12','principal component 13'])\n",
    "#principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(s)\n",
    "plt.ylabel('PCA explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logisticRegr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.score(X_test, y_test)\n",
    "y_pred = logisticRegr.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier9 = SVC(C=100,kernel='rbf',gamma='scale')\n",
    "svclassifier9.fit(train_img, y_train)\n",
    "\n",
    "y_pred = svclassifier9.predict(test_img)\n",
    "\n",
    "f1_score(y_test, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svclassifier for SVC(C=1,kernel='rbf',gamma='scale')\n",
    "#svclassifier2 = SVC(C=10,kernel='rbf',gamma='scale')\n",
    "#svclassifier3 = SVC(C=50,kernel='rbf',gamma='scale')\n",
    "#svclassifier4 = SVC(C=100,kernel='rbf',gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve,auc,r2_score\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n",
    "plt.title(\"CONFUSION MATRIX\",fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicting_probabilites = logisticRegr.predict_proba(X_test)[:,1]\n",
    "fpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\n",
    "plt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\n",
    "plt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
